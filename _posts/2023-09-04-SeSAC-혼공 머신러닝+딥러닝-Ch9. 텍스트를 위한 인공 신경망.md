---
layout: post
title: "[SeSAC]혼공 머신러닝+딥러닝 Chap9. 텍스트를 위한 인공 신경망"
date: 2023-09-04 16:06 +0900
categories:
  - SeSAC
  - 혼공 머신러닝+딥러닝
tags: []
math: true
---
## 09-1 순차 데이터와 순환 신경망

> 벡터: 크기가 있는 스칼라에 방향을 추가함으로써 같은 크기임에도 다른 숫자로 표현 가능하게 한다


### 순환 신경망(RNN: Recurrent Neural Network)

> weighted-sum(가중치합): perceptron이 계산하는것

N:1 구조
- 입력 여러개: 출력 1개

autoencoder: encode & decode
- N:1 진행 후 1:M로 나오게 함
- N과 M이 달라도 된다

## 09-2 순환 신경망으로 IMDB 리뷰 분류하기


```python
# 실행마다 동일한 결과를 얻기 위해 케라스에 랜덤 시드를 사용하고 텐서플로 연산을 결정적으로 만듭니다.
import tensorflow as tf

tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()
```

### IMDB 리뷰 데이터셋


```python
from tensorflow.keras.datasets import imdb

(train_input, train_target), (test_input, test_target) = imdb.load_data(
    num_words=300)
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
    17464789/17464789 [==============================] - 0s 0us/step

> num_words=300: 문장 길이 300이라는 뜻이 아니라 300개의 어휘를 사용한다는 뜻

```python
print(train_input.shape, test_input.shape)
```

    (25000,) (25000,)



```python
print(len(train_input[0]))
```

    218



```python
print(len(train_input[1]))
```

    189



```python
print(train_input[0])
```

    [1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 2, 284, 5, 150, 4, 172, 112, 167, 2, 2, 2, 39, 4, 172, 2, 2, 17, 2, 38, 13, 2, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 2, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 2, 12, 8, 2, 8, 106, 5, 4, 2, 2, 16, 2, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 2, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 7, 4, 2, 2, 13, 104, 88, 4, 2, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 2, 26, 2, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]



```python
print(train_target[:20])
```

    [1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]



```python
from sklearn.model_selection import train_test_split

train_input, val_input, train_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42)
```


```python
import numpy as np

lengths = np.array([len(x) for x in train_input])
```


```python
print(np.mean(lengths), np.median(lengths))
```

    239.00925 178.0



```python
import matplotlib.pyplot as plt

plt.hist(lengths)
plt.xlabel('length')
plt.ylabel('frequency')
plt.show()
```


![](https://i.imgur.com/j751kxS.png)


> 롱테일, 파레토 그래프

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

train_seq = pad_sequences(train_input, maxlen=100)
```

> 100단어 이상으로 넘어가면 잘라주고, 100단어 이하면 0으로 채워줌

```python
print(train_seq.shape)
```

    (20000, 100)



```python
print(train_seq[0])
```

    [ 10   4  20   9   2   2   2   5  45   6   2   2  33 269   8   2 142   2
       5   2  17  73  17 204   5   2  19  55   2   2  92  66 104  14  20  93
      76   2 151  33   4  58  12 188   2 151  12 215  69 224 142  73 237   6
       2   7   2   2 188   2 103  14  31  10  10   2   7   2   5   2  80  91
       2  30   2  34  14  20 151  50  26 131  49   2  84  46  50  37  80  79
       6   2  46   7  14  20  10  10   2 158]



```python
print(train_input[0][-10:])
```

    [6, 2, 46, 7, 14, 20, 10, 10, 2, 158]

> `pad_sequences`할때 뒤를 자른게 아니라 앞을 자름

```python
print(train_seq[5])
```

    [  0   0   0   0   1   2 195  19  49   2   2 190   4   2   2   2 183  10
      10  13  82  79   4   2  36  71 269   8   2  25  19  49   7   4   2   2
       2   2   2  10  10  48  25  40   2  11   2   2  40   2   2   5   4   2
       2  95  14 238  56 129   2  10  10  21   2  94   2   2   2   2  11 190
      24   2   2   7  94 205   2  10  10  87   2  34  49   2   7   2   2   2
       2   2 290   2  46  48  64  18   4   2]



```python
val_seq = pad_sequences(val_input, maxlen=100)
```

### 순환 신경망 만들기


```python
from tensorflow import keras

model = keras.Sequential()

model.add(keras.layers.SimpleRNN(8, input_shape=(100, 300))) # 100: 단어 개수 300: 벡터 개수=가중치 개수
model.add(keras.layers.Dense(1, activation='sigmoid'))
```


```python
train_oh = keras.utils.to_categorical(train_seq)
```

`to_categorical`: one-hot encoding

```python
print(train_oh.shape)
```

    (20000, 100, 300)



```python
print(train_oh[0][0][:12])
```

    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]



```python
print(np.sum(train_oh[0][0]))
```

    1.0



```python
val_oh = keras.utils.to_categorical(val_seq)
```


```python
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     simple_rnn (SimpleRNN)      (None, 8)                 2472      
                                                                     
     dense (Dense)               (None, 1)                 9         
                                                                     
    =================================================================
    Total params: 2,481
    Trainable params: 2,481
    Non-trainable params: 0
    _________________________________________________________________


### 순환 신경망 훈련하기


```python
rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer=rmsprop, loss='binary_crossentropy',
              metrics=['accuracy'])

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5',
                                                save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,
                                                  restore_best_weights=True)

history = model.fit(train_oh, train_target, epochs=100, batch_size=64,
                    validation_data=(val_oh, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```

    Epoch 1/100
    313/313 [==============================] - 31s 81ms/step - loss: 0.7003 - accuracy: 0.5002 - val_loss: 0.6970 - val_accuracy: 0.5058
    Epoch 2/100
    313/313 [==============================] - 24s 78ms/step - loss: 0.6956 - accuracy: 0.5123 - val_loss: 0.6946 - val_accuracy: 0.5124
    Epoch 3/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.6917 - accuracy: 0.5282 - val_loss: 0.6909 - val_accuracy: 0.5318
    Epoch 4/100
    313/313 [==============================] - 24s 77ms/step - loss: 0.6844 - accuracy: 0.5549 - val_loss: 0.6833 - val_accuracy: 0.5690
    Epoch 5/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.6784 - accuracy: 0.5778 - val_loss: 0.6797 - val_accuracy: 0.5770
    Epoch 6/100
    313/313 [==============================] - 24s 75ms/step - loss: 0.6733 - accuracy: 0.5892 - val_loss: 0.6753 - val_accuracy: 0.5856
    Epoch 7/100
    313/313 [==============================] - 23s 75ms/step - loss: 0.6679 - accuracy: 0.6072 - val_loss: 0.6696 - val_accuracy: 0.5980
    Epoch 8/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.6616 - accuracy: 0.6195 - val_loss: 0.6633 - val_accuracy: 0.6104
    Epoch 9/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.6539 - accuracy: 0.6312 - val_loss: 0.6568 - val_accuracy: 0.6200
    Epoch 10/100
    313/313 [==============================] - 23s 75ms/step - loss: 0.6462 - accuracy: 0.6448 - val_loss: 0.6474 - val_accuracy: 0.6372
    Epoch 11/100
    313/313 [==============================] - 23s 72ms/step - loss: 0.6375 - accuracy: 0.6571 - val_loss: 0.6414 - val_accuracy: 0.6506
    Epoch 12/100
    313/313 [==============================] - 23s 74ms/step - loss: 0.6282 - accuracy: 0.6684 - val_loss: 0.6304 - val_accuracy: 0.6614
    Epoch 13/100
    313/313 [==============================] - 23s 75ms/step - loss: 0.6194 - accuracy: 0.6791 - val_loss: 0.6212 - val_accuracy: 0.6740
    Epoch 14/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.6094 - accuracy: 0.6877 - val_loss: 0.6124 - val_accuracy: 0.6842
    Epoch 15/100
    313/313 [==============================] - 24s 77ms/step - loss: 0.5996 - accuracy: 0.7000 - val_loss: 0.6027 - val_accuracy: 0.6918
    Epoch 16/100
    313/313 [==============================] - 24s 75ms/step - loss: 0.5903 - accuracy: 0.7082 - val_loss: 0.5928 - val_accuracy: 0.6978
    Epoch 17/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5818 - accuracy: 0.7134 - val_loss: 0.5849 - val_accuracy: 0.7026
    Epoch 18/100
    313/313 [==============================] - 23s 75ms/step - loss: 0.5731 - accuracy: 0.7204 - val_loss: 0.5782 - val_accuracy: 0.7106
    Epoch 19/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5647 - accuracy: 0.7261 - val_loss: 0.5691 - val_accuracy: 0.7162
    Epoch 20/100
    313/313 [==============================] - 24s 77ms/step - loss: 0.5570 - accuracy: 0.7321 - val_loss: 0.5623 - val_accuracy: 0.7204
    Epoch 21/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5510 - accuracy: 0.7337 - val_loss: 0.5561 - val_accuracy: 0.7254
    Epoch 22/100
    313/313 [==============================] - 24s 78ms/step - loss: 0.5445 - accuracy: 0.7391 - val_loss: 0.5517 - val_accuracy: 0.7260
    Epoch 23/100
    313/313 [==============================] - 25s 79ms/step - loss: 0.5382 - accuracy: 0.7420 - val_loss: 0.5477 - val_accuracy: 0.7296
    Epoch 24/100
    313/313 [==============================] - 24s 77ms/step - loss: 0.5336 - accuracy: 0.7470 - val_loss: 0.5423 - val_accuracy: 0.7320
    Epoch 25/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5300 - accuracy: 0.7472 - val_loss: 0.5383 - val_accuracy: 0.7340
    Epoch 26/100
    313/313 [==============================] - 23s 74ms/step - loss: 0.5261 - accuracy: 0.7509 - val_loss: 0.5358 - val_accuracy: 0.7370
    Epoch 27/100
    313/313 [==============================] - 23s 74ms/step - loss: 0.5223 - accuracy: 0.7531 - val_loss: 0.5326 - val_accuracy: 0.7394
    Epoch 28/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5194 - accuracy: 0.7552 - val_loss: 0.5321 - val_accuracy: 0.7390
    Epoch 29/100
    313/313 [==============================] - 24s 77ms/step - loss: 0.5160 - accuracy: 0.7574 - val_loss: 0.5453 - val_accuracy: 0.7302
    Epoch 30/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5134 - accuracy: 0.7582 - val_loss: 0.5268 - val_accuracy: 0.7430
    Epoch 31/100
    313/313 [==============================] - 23s 75ms/step - loss: 0.5115 - accuracy: 0.7595 - val_loss: 0.5254 - val_accuracy: 0.7450
    Epoch 32/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5092 - accuracy: 0.7615 - val_loss: 0.5248 - val_accuracy: 0.7450
    Epoch 33/100
    313/313 [==============================] - 24s 77ms/step - loss: 0.5071 - accuracy: 0.7620 - val_loss: 0.5239 - val_accuracy: 0.7424
    Epoch 34/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5047 - accuracy: 0.7627 - val_loss: 0.5239 - val_accuracy: 0.7430
    Epoch 35/100
    313/313 [==============================] - 24s 75ms/step - loss: 0.5040 - accuracy: 0.7640 - val_loss: 0.5217 - val_accuracy: 0.7472
    Epoch 36/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5023 - accuracy: 0.7645 - val_loss: 0.5195 - val_accuracy: 0.7456
    Epoch 37/100
    313/313 [==============================] - 24s 76ms/step - loss: 0.5008 - accuracy: 0.7658 - val_loss: 0.5200 - val_accuracy: 0.7474
    Epoch 38/100
    313/313 [==============================] - 24s 78ms/step - loss: 0.4994 - accuracy: 0.7647 - val_loss: 0.5198 - val_accuracy: 0.7484
    Epoch 39/100
    313/313 [==============================] - 25s 80ms/step - loss: 0.4982 - accuracy: 0.7648 - val_loss: 0.5206 - val_accuracy: 0.7472



```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```


![](https://i.imgur.com/fNSXtJ1.png)



### 단어 임베딩을 사용하기


```python
model2 = keras.Sequential()

model2.add(keras.layers.Embedding(300, 16, input_length=100))
model2.add(keras.layers.SimpleRNN(8))
model2.add(keras.layers.Dense(1, activation='sigmoid'))

model2.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding (Embedding)       (None, 100, 16)           4800      
                                                                     
     simple_rnn_1 (SimpleRNN)    (None, 8)                 200       
                                                                     
     dense_1 (Dense)             (None, 1)                 9         
                                                                     
    =================================================================
    Total params: 5,009
    Trainable params: 5,009
    Non-trainable params: 0
    _________________________________________________________________



```python
rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model2.compile(optimizer=rmsprop, loss='binary_crossentropy',
               metrics=['accuracy'])

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5',
                                                save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,
                                                  restore_best_weights=True)

history = model2.fit(train_seq, train_target, epochs=100, batch_size=64,
                     validation_data=(val_seq, val_target),
                     callbacks=[checkpoint_cb, early_stopping_cb])
```

    Epoch 1/100
    313/313 [==============================] - 42s 127ms/step - loss: 0.6893 - accuracy: 0.5351 - val_loss: 0.6706 - val_accuracy: 0.5872
    Epoch 2/100
    313/313 [==============================] - 36s 116ms/step - loss: 0.6399 - accuracy: 0.6467 - val_loss: 0.6234 - val_accuracy: 0.6664
    Epoch 3/100
    313/313 [==============================] - 36s 114ms/step - loss: 0.6051 - accuracy: 0.6941 - val_loss: 0.6003 - val_accuracy: 0.6948
    Epoch 4/100
    313/313 [==============================] - 37s 118ms/step - loss: 0.5831 - accuracy: 0.7172 - val_loss: 0.5888 - val_accuracy: 0.7026
    Epoch 5/100
    313/313 [==============================] - 37s 117ms/step - loss: 0.5663 - accuracy: 0.7305 - val_loss: 0.5669 - val_accuracy: 0.7300
    Epoch 6/100
    313/313 [==============================] - 36s 114ms/step - loss: 0.5527 - accuracy: 0.7408 - val_loss: 0.5536 - val_accuracy: 0.7356
    Epoch 7/100
    313/313 [==============================] - 37s 118ms/step - loss: 0.5410 - accuracy: 0.7475 - val_loss: 0.5422 - val_accuracy: 0.7404
    Epoch 8/100
    313/313 [==============================] - 36s 116ms/step - loss: 0.5313 - accuracy: 0.7509 - val_loss: 0.5443 - val_accuracy: 0.7352
    Epoch 9/100
    313/313 [==============================] - 36s 114ms/step - loss: 0.5236 - accuracy: 0.7549 - val_loss: 0.5463 - val_accuracy: 0.7238
    Epoch 10/100
    313/313 [==============================] - 36s 115ms/step - loss: 0.5177 - accuracy: 0.7578 - val_loss: 0.5377 - val_accuracy: 0.7382
    Epoch 11/100
    313/313 [==============================] - 35s 113ms/step - loss: 0.5135 - accuracy: 0.7591 - val_loss: 0.5264 - val_accuracy: 0.7454
    Epoch 12/100
    313/313 [==============================] - 35s 112ms/step - loss: 0.5094 - accuracy: 0.7619 - val_loss: 0.5265 - val_accuracy: 0.7422
    Epoch 13/100
    313/313 [==============================] - 36s 116ms/step - loss: 0.5056 - accuracy: 0.7641 - val_loss: 0.5207 - val_accuracy: 0.7468
    Epoch 14/100
    313/313 [==============================] - 36s 114ms/step - loss: 0.5027 - accuracy: 0.7657 - val_loss: 0.5226 - val_accuracy: 0.7434
    Epoch 15/100
    313/313 [==============================] - 36s 117ms/step - loss: 0.4998 - accuracy: 0.7689 - val_loss: 0.5678 - val_accuracy: 0.6992
    Epoch 16/100
    313/313 [==============================] - 36s 116ms/step - loss: 0.4977 - accuracy: 0.7677 - val_loss: 0.5155 - val_accuracy: 0.7500
    Epoch 17/100
    313/313 [==============================] - 37s 119ms/step - loss: 0.4944 - accuracy: 0.7713 - val_loss: 0.5222 - val_accuracy: 0.7450
    Epoch 18/100
    313/313 [==============================] - 36s 115ms/step - loss: 0.4927 - accuracy: 0.7712 - val_loss: 0.5149 - val_accuracy: 0.7486
    Epoch 19/100
    313/313 [==============================] - 36s 114ms/step - loss: 0.4905 - accuracy: 0.7730 - val_loss: 0.5138 - val_accuracy: 0.7464
    Epoch 20/100
    313/313 [==============================] - 37s 117ms/step - loss: 0.4880 - accuracy: 0.7740 - val_loss: 0.5124 - val_accuracy: 0.7500
    Epoch 21/100
    313/313 [==============================] - 36s 115ms/step - loss: 0.4871 - accuracy: 0.7749 - val_loss: 0.5143 - val_accuracy: 0.7488
    Epoch 22/100
    313/313 [==============================] - 36s 114ms/step - loss: 0.4847 - accuracy: 0.7775 - val_loss: 0.5148 - val_accuracy: 0.7486
    Epoch 23/100
    313/313 [==============================] - 36s 116ms/step - loss: 0.4830 - accuracy: 0.7775 - val_loss: 0.5135 - val_accuracy: 0.7500



```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```


![](https://i.imgur.com/4mooQrF.png)

