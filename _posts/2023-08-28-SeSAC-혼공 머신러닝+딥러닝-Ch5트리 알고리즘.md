---
layout: post
title: "[SeSAC]혼공 머신러닝+딥러닝 Chapter 5. 트리 알고리즘"
date: 2023-08-28 09:09 +0900
categories: [SeSAC, 머신러닝 데이터분석]
tags: []
math: true
---

## 05-1 결정 트리

---

### Information Gain (정보 이득)

**정의:** 
Information Gain은 어떤 속성을 기준으로 데이터를 분할했을 때 얻을 수 있는 엔트로피의 감소량이다. 결정 트리 알고리즘에서는 Information Gain이 최대가 되는 속성으로 데이터를 분할한다.

**수식:** 
$$
\text{Information Gain} = \text{Entropy(parent)} - \sum \left( \frac{\text{number of samples in child}}{\text{number of samples in parent}} \times \text{Entropy(child)} \right)
$$

**엔트로피 수식:** 
$$
\text{Entropy}(S) = -p_+ \log_2(p_+) - p_- \log_2(p_-)
$$
여기서 \( p_+ \)는 양성 샘플의 비율, \( p_- \)는 음성 샘플의 비율이다.

---

### Gini Impurity (지니 불순도)

**정의:** 
Gini Impurity는 임의로 선택된 샘플이 잘못 분류될 확률을 측정한다. 결정 트리 알고리즘에서는 Gini Impurity가 낮은 속성으로 데이터를 분할한다.

**수식:** 
$$
\text{Gini Impurity}(S) = 1 - (p_+^2 + p_-^2)
$$
여기서 \( p_+ \)는 양성 샘플의 비율, \( p_- \)는 음성 샘플의 비율이다.

---


> 모델과 알고리즘의 차이(Decision Tree는 사실 Decision Model이어야 한다?)


### 로지스틱 회귀로 와인 분류하기
```python
import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')

wine.head()

wine.info()

wine.describe()

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)

print(train_input.shape, test_input.shape)  # (5197, 3) (1300, 3)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

#### 설명하기 쉬운 모델과 어려운 모델
```python
print(lr.coef_, lr.intercept_)  # [[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]
```
### 결정 트리
```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))  # 0.996921300750433
print(dt.score(test_scaled, test_target))  # 0.8592307692307692

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```
![](https://i.imgur.com/q7RHGsL.png)

```python

plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![](https://i.imgur.com/ZgEWQvr.png)

#### 가지치기(Pruning): 과적합 줄이기

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show
```

![](https://i.imgur.com/WNvTRqG.png)


```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target))  # 0.8454877814123533
print(dt.score(test_input, test_target))  # 0.8415384615384616


plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()

print(dt.feature_importances_)  # [0.12345626 0.86862934 0.0079144 ]
```

> # Standard Scaling한것과 안한것의 차이가 거의 없음을 알 수 있음

![](https://i.imgur.com/gGDfZqt.png)

### 확인문제
```python
dt = DecisionTreeClassifier(min_impurity_decrease=0.0005, random_state=42)
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target))  # 0.8454877814123533
print(dt.score(test_input, test_target))  # 0.8415384615384616

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

> min_impurity_decrease: superset과 subset impurity차이의 최솟값 설정

![](https://i.imgur.com/TFJF0SP.png)


## 05-2 교차 검증과 그리드 서치
