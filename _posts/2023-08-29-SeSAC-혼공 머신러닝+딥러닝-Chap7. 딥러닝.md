---
layout: post
title: "[SeSAC]혼공 머신러닝+딥러닝 Chap7. 딥러닝"
date: 2023-08-30 14:23 +0900
categories: [SeSAC, 머신러닝 데이터분석]
tags: []
math: true
---
## 07-1 인공 신경망

```python
import tensorflow as tf

tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()
```

### 패션 MNIST:
- MNIST(Modified National Institute of Standars and Technology Database)

```python
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

print(train_input.shape, train_target.shape)  # (60000, 28, 28) (60000,)

print(test_input.shape, test_target.shape)  # (10000, 28, 28) (10000,)

import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')
    axs[i].axis('off')
plt.show()
```

![](https://i.imgur.com/5JVHqKJ.png)


```python
print([train_target[i] for i in range(10)])  # [9, 0, 0, 3, 0, 2, 7, 2, 5, 5]

import numpy as np

print(np.unique(train_target, return_counts=True))
# (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))
```

### 로지스틱 회귀로 패션 아이템 분류하기

```python
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)

print(train_scaled.shape)  # (60000, 784)

from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)

scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))  # 0.8196000000000001

```

### 인공신경망
#### 텐서플로와 케라스
```python
import tensorflow as tf

from tensorflow import keras
```

### 인공신경망으로 모델 만들기
```python
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled.shape, train_target.shape)  # (48000, 784) (48000,)

print(val_scaled.shape, val_target.shape)  # (12000, 784) (12000,)

dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))

model = keras.Sequential(dense)

```

### 인공신경망으로 패션 아이템 분류하기

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

print(train_target[:10])  # [7 3 5 8 6 9 3 3 9 9]

model.fit(train_scaled, train_target, epochs=5)

model.evaluate(val_scaled, val_target)

# [0.45262545347213745, 0.8464999794960022]
```


---

### Perceptron

![](https://i.imgur.com/Do2k9E6.png)


### 인공지능 관련 인물들

#### **Marvin Minsky**:

- **정보**:
    - 1927년에 태어난 미국의 컴퓨터 과학자 및 인공지능 연구자.
    - MIT 미디어 랩의 공동 창립자로, 인공지능 및 인지과학 분야에서 큰 영향을 끼침.
    - "인공지능의 아버지" 중 한 명으로 불림.
    - 심리학 및 컴퓨터 과학을 접목시켜 인간의 지능을 모델링하려는 노력을 기울임.
    - "프레임(Frame)"이라는 개념을 소개하여 지식 표현을 발전시킴.
    - '퍼셉트론(Perceptron)'의 한계를 지적하며, 기호주의 접근법을 비판함.

#### **Frank Rosenblatt**:

- **정보**:
    - 1928년에 태어난 미국의 심리학자 및 컴퓨터 과학자.
    - "퍼셉트론(Perceptron)" 개념을 개발한 인공지능의 선구자 중 한 명.
    - 퍼셉트론은 초기 인공 신경망 모델로, 단순한 이진 분류를 수행하는 모델이다.
    - 기계 학습에서 신경망을 활용한 패턴 인식의 초기 기초를 마련한 인물로 평가됨.

#### **Geoffrey Everest Hinton**:

- **정보**:
    - 1947년에 태어난 캐나다의 컴퓨터 과학자.
    - '딥 러닝' 분야의 선구자 중 한 명으로 꼽힘.
    - 역전파 알고리즘을 개발하여 다층 퍼셉트론의 학습을 가능하게 함.
    - '드롭아웃(Dropout)'과 같은 기법을 도입하여 신경망의 과적합 문제를 해결하는데 기여.
    - 이미지 인식 등 다양한 분야에서 혁신적인 딥러닝 모델을 개발하며 인공지능 분야를 선도.

#### **Yann LeCun**:

- **정보**:
    - 1960년에 태어난 프랑스 출신의 컴퓨터 과학자 및 머신러닝 연구자.
    - "딥 러닝의 아버지"로 불리며, 현대 딥러닝의 발전에 큰 역할을 한 인물 중 하나.
    - 합성곱 신경망(CNN, Convolutional Neural Network)의 개념과 구조를 개발하여 컴퓨터 비전 분야에서의 중요한 기여를 함.
    - 손글씨 숫자를 인식하는 MNIST 데이터셋을 활용한 CNN의 성공적인 적용으로 유명.
    - 뉴욕대학교 (NYU) 교수이자, Facebook 인공지능 연구부 (FAIR)의 연구원이기도 함.

### 인공지능 파벌
#### **기호주의자(Symbolist or Symbolic AI)**:
    - 기호와 기호 간의 관계에 중점을 둠.
    - 추론과 논리를 핵심적인 요소로 간주.
    - 수작업으로 시스템에 규칙과 지식을 프로그래밍하여 지능적 행동을 유도.
    - 복잡한 지식을 처리하는 데 어려움이 있음.

#### **연결주의자(Connectionist or Connectionist AI)**:
    - 뉴런과 신경망을 모방한 인공 신경망 활용.
    - 데이터와 패턴을 학습하여 시스템이 스스로 지능적 행동을 개발하도록 함.
    - 신경망을 통한 특징 추출, 패턴 학습, 문제 해결에 초점.
    - 대량의 데이터와 학습이 필요하며 복잡한 문제에 강점을 보임.


**`sparse_categorical_crossentropy`**
- `categorical_crossentropy`: skit-learn의 `'log-loss'`와 
- `sparse`: 원핫-인코딩

batch_size: (dafault: 32)
- 의미: 32번의 평균을 구해서 가중치를 수정


## 07-2 신층 신경망

### 2개의 층

```python
import tensorflow as tf

tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()

from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

from sklearn.model_selection import train_test_split

train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))
dense2 = keras.layers.Dense(10, activation='softmax')
```

### 심층 신경망 만들기
```python
model = keras.Sequential([dense1, dense2])

model.summary()
```
![](https://i.imgur.com/5kBSNLh.png)

### 층을 추가하는 다른 방법
```python
model = keras.Sequential([
    keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'),
    keras.layers.Dense(10, activation='softmax', name='output')
], name='패션 MNIST 모델')

model.summary()
```
![](https://i.imgur.com/bM5EIp9.png)

```python
model = keras.Sequential()
model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()
```
![](https://i.imgur.com/FwO1QY1.png)

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)
```
![](https://i.imgur.com/FWcTh7Q.png)

### 렐루 활성화 함수
```python
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()
```
![](https://i.imgur.com/eXea1OA.png)


```python
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)
```
![](https://i.imgur.com/7mb59AY.png)

```python
model.evaluate(val_scaled, val_target)  # [0.3683287501335144, 0.8725833296775818]
```
![](https://i.imgur.com/9G5UDrm.png)

### 옵티마이저
```python
model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')

sgd = keras.optimizers.SGD()
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')

sgd = keras.optimizers.SGD(learning_rate=0.1)

sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)

adagrad = keras.optimizers.Adagrad()
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')

rmsprop = keras.optimizers.RMSprop()
model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')

model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)
```
![](https://i.imgur.com/VNHXvRc.png)

```python

model.evaluate(val_scaled, val_target)
```

![](https://i.imgur.com/2bWWD4m.png)


####
Activation
- 히든 레이어의 activation은 성능에는 영향을 주지만 다 들어갈 수 있다
- 출력 레이어의 activation은 원하는 출력에 따라 정해야한다


딥러닝의 히든 레이어에는 비선형 activation을 써야한다: 선형을 쓸 순 있지만 의미가없음

시그모이드함수는 3번 이상 쓰이면 효과가 없어짐

Relu