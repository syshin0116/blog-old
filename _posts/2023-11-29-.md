### Attention Function

- Q = Query: t 시점의 디코더 셀에서의 은닉 상태
	- 현재 처리하고 있는 단어나 문장 부분
- K = Key: 모든 시점의 인코더 셀의 은닉 상태들
	- 비교 대상이 되는 데이터 세트에서의 요소들
- V = Values : 모든 시점의 인코더 셀의 은닉 상태들
	- 각 Key에 연관된 출력값


![](https://i.imgur.com/GFzEHdt.png)

#### 예시

'나는 학교에 간다'를 영어로 번역한다면

- Key : 번역 모델이 학습한 데이터에서 '나는 학교에 간다'와 유사한 구조나 의미를 가진 요소들
- Query: '나는 학교에 간다'라는 문장의 각 단어
- Value: '나는 학교에 간다'에 해당하는 영어 문장 구조의 요소들이 Value

- Attention Function은 이 세 요소를 사용하여 입력된 Query와 가장 관련이 높은 정보를 Key-Value 쌍에서 찾아냄. 
- 이 과정을 통해, 모델은 '나는 학교에 간다'라는 문장의 각 단어나 구문이 영어로 어떻게 번역되어야 할지를 학습한 데이터를 기반으로 결정

예를 들어, '나는'이라는 단어(Query)에 대해, 모델은 학습 데이터에서 이와 관련된 Key-Value 쌍을 찾아 'I'(Value)로 번역한다. 이러한 과정이 전체 문장에 걸쳐 이루어지며, 최종적으로 '나는 학교에 간다'는 'I go to school'로 변환된다.

이 과정에서 중요한 것은 모델이 각 단어의 맥락을 이해하고, 문장 전체의 의미를 유지하면서 적절한 번역을 찾아내는 것이다.


### Seq to Seq 의 문제점
